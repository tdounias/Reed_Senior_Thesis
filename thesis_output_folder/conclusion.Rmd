---
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(Matrix)
library(lme4)
library(gamm4)
library(pander)
library(modelr)
source("~/Desktop/Reed_Senior_Thesis/riggd/R/utils.R")
setwd("~/Desktop/Reed_Senior_Thesis/Data_and_results/data")

model_sample <- read_csv("model_indiv_sample.csv")

#DATA

##Model 1 Gibbs
load("all_turnouts.RData")
demographics <- read_csv("colorado_demographic_stats_by_county.csv")
names(demographics) <- tolower(names(demographics))

model_dt <- turnouts_county_data(turnout_list)

model_dt$dates <- as.factor(model_dt$dates)
model_dt$types <- as.factor(model_dt$types)

model_dt <- filter(model_dt, !is.na(model_dt$reg))

model_dt <- left_join(model_dt, demographics, by = "county")

model_dt$county <- as.factor(model_dt$county)

cnt_vec <- as.character(unique(model_dt$county))

##Model 2 Gibbs
U <- unique(data.frame(model_dt$pct_urban, model_dt$pct_white))
names(U) <- c("urban", "white")
```

# Conclusion {.unnumbered}
  \setcounter{chapter}{5}
	\setcounter{section}{0}

This thesis started off with the prospect of testing three hypotheses, on the impact of VBM on turnout (H1) and how that impact may differ based on how prominent national effects are during election time (H2), specifically using the metric of election type between Midterm, General, Primary, and Coordinated elections (H3). These hypotheses derived from a theory of voting "at the margins", first developed by Aldrich (1993), which stated that the decision on whether to turn out or not is based on a rational calculus made by individual voters. This calculus is affected by a wide range of effects, from small local ones to large, national variables. In my first Chapter I identified how this theory fits in the general literature on voter behavior, and made a series of observations on how each theory views the impact of VBM. The hypotheses were then tested on Voter Files from Colorado, which described registration and voter history for elections ranging from 2010-2016. The process of data wrangling, modeling, and estimations revealed that the data I used was not ideal for the purposes I intended, particularly for fitting individual-level models.

At the end of my thesis I am able to confirm H1, but reject H2 and H3. These results come from my County level models. Despite non-convergence my individual level models have some inferential potential, but they are still not particularly useful at testing my hypotheses on VBM. This is true because of the variability concerns outlined early in Chapter 4. I confirm H1 because mail voting seems to be an incremental, comparably small effect on turnout. I reject H2/H3 because that effect is not more pronounced for elections that have less strong national influences. In fact, my county-level models showed the exact opposite: VBM had a strong, significant positive effect for presidential elections, but no discernible effect for primary, coordinated races, or midterm races. 

This is evidence against voting "at the margins", since my hypotheses that were most connected to Aldrich's model (H2, H3) were convincingly rejected. I find evidence against a hypothesis of habitual voting as well, since in my county models Mail Voting has a significant, positive effect. This means that the absence of election-day effects or the strong presence of habitual voting did not override the effect on turnout that mail voting has. The same conclusion can be drawn for a social theory of voting, which predicts no effect.

Interpreting my results through a resources and organizational lens seems on the surface more convincing, since this theory predicts a consistent, positive effect of VBM on turnout because of the increase in capacity it gives to individuals. However the effect is not comparativelly large, as demonstrated by the fact that I confirm H1. It should also be noted that this effect should be present in *all* elections to be convincing evidence for the resources electoral participation paradigm. This is not the case. One explanation may be that my models were sensitive to the vastly greater turnout that occurs during presidential and midterm years, particularly since the differences in mail voting percentage between counties was at most around 20%. This, again, leads to the conclusion that the lack of variability in my data does not permit any more precise analysis.

The first step towards future research, directly resulting from this problem, would be to get more data. This is possible, but a task I was unable to accomplish due to time constraints related to this project. More data would allow for a comparative study of Colorado elections before and after all major legislative electoral changes of this century (2008, 2013). Another way to expand on my research would be to look at even lower level elections, like municipal elections, school board elections, or recall elections; voter history files contain data on all of these races. A study with more data could use such races as well for testing the hypotheses I set up in this thesis. Third, it is possible to replicate my research here for other states like Oregon and Washington, or for the specific counties in Arizona, Utah, or California that have all-mail elections. Fourth, elections thankfully do not stop happening; my research here can always be updated with data from the 2017 Coordinated Colorado election, or the 2018 Midterm that just took place. I would caution that the same issue with data variability would still exist in this case; there is a much greater need for data going back, rather than new all-mail elections. 

Apart from expanding my research in terms of data, another path would be to implement some of the methods I outline in the beginning of Chapter 4, like the Synthetic Control Group. Such methods would allow for inferences to be made despite the data issues I encountered. Lastly, it should be noted that VBM is just one of a series of electoral reforms like Automatic Voter Registration (AVR), Early Voting, or Voter ID Restrictions, all of which can potentially be tested using multilevel models and data wrangling methods that I have applied in this thesis. 
  
Another contribution of my thesis is my analysis of data wrangling, the construction of multilevel general additive models for turnout, and the accompanying \textit{R} package. To take these one by one, I have provided arguments in favor of preferring multiple snapshots of registration files rather than just the latest iteration of the record. I have analyzed the pitfalls that exist in such documents, and given specific examples on how this can be dealt with for the Colorado data files. I have also provided a set of variable specification that can be useful as indicators of the content of these data, or the potential uses of voter registration files in other future studies. Finally, I have presented potential future solutions to issues these data have with variability, and ways to circumvent processing power limitations.
   
Additionally, I have meticulously gone through the creation of multilevel general additive models of individual and county level turnout. While due to data and processing power limitations I am unable to run all these models to typical standards of convergence. This does not mean that they present no value to future research. Quite the contrary, future researchers just have to go through the data clean-up stage, and then implement my models without having to construct them from scratch. In particular, mixed effects and general additive models are not widely used in such studies, making their presentation and specification rather unique regardless of their application in this piece of research.  
  
Lastly, I provide an extensive library of code used to create this document and the research I conduct. I have made an \textit{R} package--which I named `riggd`--that includes more than a dozen different functions that serve data wrangling and presentation purposes. These functions are made for use on Colorado files, but require relatively small amounts of changes to be applied to voter files from around the US. I also provide code for all tables and graphics that are included in this thesis on gitHub, which is a testament to the reproducibility and future value of the research I conducted.  

I recognize that, despite many obstacles in terms of data or computing power, the outcome of this thesis being more constructive rather than conclusive is to some extent my fault. There were many problems in this thesis that I should have been aware of earlier in the process, which may have allowed me to present more concrete results rather than a series of tools and methods. However, in the combination of my existing conclusions and the materials I have created through this process, it is my belief that this thesis does in fact present a step forward in the literature, and that it adds to existing quantitative elections studies works. This is a small step, but it helps in our understanding of how voters behave, what the actual results of election policy are, and how to expand participation.

<!--
If you feel it necessary to include an appendix, it goes here.
-->

\appendix

# MCMC Estimation Processes for Multilevel Models

In statistical science a Markov Chain is a sequence of random variables whose value depends on the value of the exact previous random variable. In mathematical terms, this would be a sequence $\theta^{(1)}, \theta^{(2)}, \theta^{(3)}, ..., \theta^{(t)}$ where $\mathbb{P}(\theta^{(t)} = y|\theta^{(n)}, n<t) = \mathbb{P}(\theta^{(t)} = y|\theta^{(t-1)})$. A Markov Chain Monte Carlo simulation uses Bayesian estimation to update each sequential estimate of $\theta$, leading it to converge to the true value being estimated [@gelman_data_2006; @jackman_bayesian_2009].  

Multilevel models can be estimated using MCMC sampling. Indicatively, this appendix presents the construction and coding of two types of MCMC samplers based on the Gibbs algorithm. The code and mathematical derivations are adapted to my models from Gelman and Hill (2006).

## Gibbs Sampler for the County Models
  
The Gibbs algorithm works as follows:

\begin{enumerate}
  \item Choose a number of parallel simulation runs (chains). This number should be relatively low. In this example it is set to 3.
  \item For each chain do the following:
  \begin{enumerate}
    \item Initialize vector of parameters $\Theta^{(0)} = \{\theta^{(0)}_1\, \theta^{(0)}_2\, ..., \theta^{(0)}_n\}$
    \item Choose a number of iterations. For each iteration update every parameter in vector $\Theta^{(n_{iteration})}$, based on the values of vector $\Theta^{(n_{iteration} - 1)}$.
  \end{enumerate}
  \item Evaluate convergence between the chains.
\end{enumerate}

If convergence is poor, repeat for more iterations, or follow diagnostic procedures. These are not specified here, but Gelman and Hill provide a good overview [@gelman_data_2006; @gelman_bayesian_2003]. 

### County Model 1 (Only Random County Effects)

A basic multilevel model with only group-level intercept mixed effects can be written as follows:

$$y_i \ \sim \ N(a_{j[i]}, \sigma^2_y),\ i \in [1, n] \\ a_j \ \sim \ N(\mu_{\alpha}, \sigma^2_{\alpha}), \ j \in [1,J]$$
This specification is slightly different from that presented in Chapter 2. Here $\alpha_{j[i]}$ is the coefficient for the group $j$ that individual $i$ belongs to, $\sigma_y, \sigma_{\alpha}$ the variances of the individual and group level distributions respectively, and $\mu_{\alpha}$ the mean of the group-level distribution. In the case of the most basic county-level model estimated in my thesis (County Model 1), $n = 704$ and $J = 64$. Using Maximum Likelihood Estimation, and given that:

\begin{equation}
  \alpha_j|y, \mu_{\alpha}, \sigma_y, \sigma_{\alpha} \ \sim \ N(\hat{\alpha_j}, V_j)
\end{equation}

we can obtain estimates:

\begin{equation}
\hat{\alpha_j} = \frac{\frac{n_{[j]}}{\sigma^2_y}\bar{y}_{[j]} + \frac{1}{\sigma^2{\alpha}}}{\frac{n_{[j]}}{\sigma^2_y} + \frac{1}{\sigma^2{\alpha}}},\ \ \ \ \  V_j = \frac{1}{\frac{n_{[j]}}{\sigma^2_y} + \frac{1}{\sigma^2{\alpha}}},
\end{equation}

where $n_{[j]}$ is the number of observations for group j, and $\bar{y}_{[j]}$ is the mean response for group j. Using these estimates and the common MLE estimates for variance and mean in a normal distribution, it is possible to construct a Gibbs sampler for model coefficients and errors. Step 2(b) in the Gibbs sampler would then be:  
  
\begin{enumerate}
  \item Estimate $a_j, \ j\in[1,J]$ using equations (1), (2).
  \item Estimate $\mu_{\alpha}$ by drawing from $N(\frac{1}{J}\sum_{1}^{J}\alpha_j, \sigma_{\alpha}^2/J)$ using the previous values estimated in step 1.
  \item Estimate $\sigma_y^2$ as $\frac{\frac{1}{n}\sum_{1}^{n}(y_i - \alpha_{j[i]})^2}{X_{n-1}^2}$ where $X_{n-1}^2$ is a draw from a $\chi^2$ distribution with $n-1$ degrees of freedom.
  \item Estimate $\sigma_{\alpha}^2$ as $\frac{\frac{1}{J}\sum_{1}^{J}(\alpha_j - \mu_{\alpha})^2}{X_{J-1}^2}$ where $X_{n-1}^2$ is a draw from a $\chi^2$ distribution with $J-1$ degrees of freedom.
\end{enumerate}

While each step here seems relatively intuitive, the derivations behind some of the details (like the chi-squared distribution) are complex MLE processes and beyond the scope of this thesis. The R code for this algorithm is as follows:

```{r}
## Gibbs sampler in R
a.update <- function(){
  a.new <- rep (NA, J)
  for (j in 1:J){
    n.j <- sum (model_dt$county==cnt_vec[j])
    y.bar.j <- mean (model_dt$turnout[model_dt$county==cnt_vec[j]])
    a.hat.j <- ((n.j/sigma.y^2)*y.bar.j + (1/sigma.a^2)*mu.a)/
               (n.j/sigma.y^2 + 1/sigma.a^2)
    V.a.j <- 1/(n.j/sigma.y^2 + 1/sigma.a^2)
    a.new[j] <- rnorm (1, a.hat.j, sqrt(V.a.j))
  }
  return (a.new)
}
mu.a.update <- function(){
  mu.a.new <- rnorm (1, mean(a), sigma.a/sqrt(J))
  return (mu.a.new)
}
sigma.y.update <- function(){
  sigma.y.new <- sqrt(sum((model_dt$turnout-
                             a[model_dt$county])^2)/rchisq(1,703))
  return (sigma.y.new)
}
sigma.a.update <- function(){
  sigma.a.new <- sqrt(sum((a-mu.a)^2)/rchisq(1,J-1))
  return (sigma.a.new)
}

J <- 64
n.chains <- 3
n.iter <- 1000
sims <- array (NA, c(n.iter, n.chains, J+3))
dimnames (sims) <- list (NULL, NULL, 
                         c (paste ("a[", 1:J, "]", sep=""), "mu.a",
   "sigma.y", "sigma.a"))

for (m in 1:n.chains){
  mu.a <- rnorm (1, mean(model_dt$turnout), sd(model_dt$turnout))
  sigma.y <- runif (1, 0, sd(model_dt$turnout))
  sigma.a <- runif (1, 0, sd(model_dt$turnout))
  for (t in 1:n.iter){
    a <- a.update ()
    mu.a <- mu.a.update ()
    sigma.y <- sigma.y.update ()
    sigma.a <- sigma.a.update ()
    sims[t,m,] <- c (a, mu.a, sigma.y, sigma.a)
  }
}
```

```{r, echo = FALSE, results = "asis", message = FALSE}
#Get Values after convergance
vals <- data.frame(rep(0,3))
for(i in 1:67){
  for(j in 1:3){
    vals[j,i] <- mean(sims[-c(1:500),j,i])
  }
}

vals <- vals %>%
  summarise_all(mean)

names(vals) <- c (paste ("a[", 1:J, "]", sep=""), "mu.a",
   "sigma.y", "sigma.a")


est_table <- data.frame(c("Sampler", "Model"), 
                        rbind(vals[1,65:67], c(0.469, 0.199, 0.039)))

names(est_table)[1] <- "Calculated from..."

pandoc.table(est_table, justify = c("left", "center", "center", "center"), 
  caption = "Gibbs sampler results for County Model 1 \\label{tab:gibbs_1}")
```

As is obvious from Table, the Gibbs sampler produces values very similar to the ones given by an \textit{R} call of Model 1.

### County Model 2 (Random County Effects and County-Level Predictors)

With slight changes from the previous model the following is the mathematical expression for a mixed effects model with group-level predictors:

$$y_i \ \sim \ N(a_{j[i]}, \sigma^2_y),\ i \in [1, n] \\ a_j \ \sim \ N(U_j\gamma, \sigma^2_{\alpha}), \ j \in [1,J], $$

where $U_j$ is a vector of predictor values for group $j$, and $\gamma$ a vector of group-level coefficients, with the rest of the parameters having the same designation as previously. Bear in mind that the second of the previous expressions can also be written as: 

\begin{equation}
\alpha_j = U_j\gamma + \eta_j, \ \ \eta_j \ \sim \ N(0, \sigma_{\alpha}^2)
\end{equation}


Updating the estimates used previously, it is again possible to construct a Gibbs sampler for model coefficients and errors. Step 2(b) in the Gibbs sampler in this case is:

\begin{enumerate}
  \item Estimate $a_j, \ j\in[1,J]$. Start by calculating $y_i^{temp} = y_i - U_{j[i]}\gamma$. Then calculate an estimate $\hat\eta_j$ and variance matrix $V_j$ from equations (1), (2), by replacing $\hat\alpha_j$ with $\hat\eta_j$ and $y$ with $y^{temp}$. Use $\eta_j \ \sim \ N(\hat\eta_j, V_j)$ to draw errors $\eta_j$ and then use (3) to estimate $\alpha_j$ for $j \in [1,J]$.
  \item Estimate $\gamma$ by first regressing $\alpha$ by predictor matrix $U$ to obtain $\hat\gamma$ and variance matrix $V_{\gamma}$. Then use distribution $\gamma_j \ \sim \ N(\hat\gamma_j, V_j)$ to obtain estimates for vector $\gamma$.
  \item Estimate $\sigma_y^2$ as $\frac{\frac{1}{n}\sum_{1}^{n}(y_i - \alpha_{j[i]})^2}{X_{n-1}^2}$ where $X_{n-1}^2$ is a draw from a $\chi^2$ distribution with $n-1$ degrees of freedom.
  \item Estimate $\sigma_{\alpha}^2$ as $\frac{\frac{1}{J}\sum_{1}^{J}(\alpha_j - U_j\gamma)^2}{X_{J-1}^2}$ where $X_{n-1}^2$ is a draw from a $\chi^2$ distribution with $J-1$ degrees of freedom.
\end{enumerate}

County Model 2, as presented in Chapter 4, includes two county-level predictors: percentage of white residents and percentage of urban population; this means that $U = \{x^{\%white}, x^{\%urban}\}$. Keeping this in mind the following code estimates the coefficients and standard errors for Model 2:

```{r}
## Gibbs sampler for a multilevel model with county predictors
  

a.update <- function(){
  y.temp <- model_dt$turnout - 
    (model_dt$pct_urban*g[1] + model_dt$pct_white*g[2])
  eta.new <- rep (NA, J)
  for (j in 1:J){
    n.j <- sum (model_dt$county==cnt_vec[j])
    y.bar.j <- mean (y.temp[model_dt$county==cnt_vec[j]])
    eta.hat.j <- ((n.j/sigma.y^2)*y.bar.j/
                 (n.j/sigma.y^2 + 1/sigma.a^2))
    V.eta.j <- 1/(n.j/sigma.y^2 + 1/sigma.a^2)
    eta.new[j] <- rnorm (1, eta.hat.j, sqrt(V.eta.j))
  }
  a.new <- (U$urban*g[1] + U$white*g[2]) + eta.new
  return (a.new)
}

g.update <- function(){
  lm.0 <- lm (a ~ U$urban + U$white)
  g.new <- coef (lm.0)[2:3]
  return (g.new)
}

sigma.y.update <- function(){
  sigma.y.new <- sqrt(sum((model_dt$turnout-
                a[model_dt$county])^2)/rchisq(1,703))
  return (sigma.y.new)
}

sigma.a.update <- function(){
  sigma.a.new <- sqrt(sum((a-(model_dt$pct_urban*g[1] + 
                model_dt$pct_white*g[2]))^2)/rchisq(1,J-1))
  return (sigma.a.new)
}

J <- 64
n.chains <- 3
n.iter <- 2000
sims <- array (NA, c(n.iter, n.chains, J+4))
dimnames (sims) <- list (NULL, NULL, c (paste ("a[", 1:J, "]", sep=""), 
   c("coef.urban", "coef.white"),
   "sigma.y", "sigma.a"))

for (m in 1:n.chains){
  g <- rnorm (2)
  sigma.y <- runif (1, 0, sd(model_dt$turnout))
  sigma.a <- runif (1, 0, sd(model_dt$turnout))
  for (t in 1:n.iter){
    a <- a.update ()
    g <- g.update ()
    sigma.y <- sigma.y.update ()
    sigma.a <- sigma.a.update ()
    sims[t,m,] <- c (a, g, sigma.y, sigma.a)
  }
}
```

```{r, echo = FALSE, results = "asis", message = FALSE}
#Get Values after convergance
vals <- data.frame(rep(0,3))
for(i in 1:68){
  for(j in 1:3){
    vals[j,i] <- mean(sims[-c(1:1000),j,i])
  }
}

vals <- vals %>%
  summarise_all(mean)

names(vals) <- c (paste ("a[", 1:J, "]", sep=""), 
                  "coef_urban", "coef_white",
                  "sigma.y", "sigma.a")


est_table <- data.frame(c("Sampler", "Model"),
rbind(vals[1,65:68], c(-0.118, 0.034, 0.199, 2.631)))

names(est_table)[1] <- "Calculated from..."

pandoc.table(est_table, 
      justify = c("left", "center", "center", "center", "center"),
      caption = "Gibbs sampler results for County Model 2 \\label{tab:gibbs_2}")
```
   
As previously the values outputted by the Gibbs sampler are very close to those estimated by the model, apart from the group level standard deviation. Some variability in how closely the sampler approximates the model call is to be expected, due to the difference in how the model is estimated in R (much more precise Bayesian processes).