\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{}
    \pretitle{\vspace{\droptitle}}
  \posttitle{}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  

\begin{document}

\section*{Conclusion}\label{conclusion}
\addcontentsline{toc}{section}{Conclusion}

\setcounter{chapter}{5} \setcounter{section}{0}

At the end of my thesis I am able to confirm H1, but reject H2 and H3.
These results come from my County level models. Despite non-convergence
my individual level models have some inferential potential, but they are
still not particularly useful at testing my hypotheses on VBM. This is
true because of the variability concerns outlined early in Chapter 4. I
confirm H1 because mail voting seems to be an incremental, comparably
small effect on turnout. I reject H2/H3 because that effect is not more
pronounced for elections that have less strong national influences. In
fact, my county-level models showed the exact opposite: VBM had a
strong, significant positive effect for midterm and presidential
elections, but no discernible effect for primary or coordinated races.

This is evidence against voting ``at the margins'', since my hypotheses
that were most connected to Aldrich's paradigm (H2, H3) were
convincingly rejected. I find evidence against a hypothesis of habitual
voting as well, since in my county models Mail Voting has a significant,
positive effect. This means that the absence of election-day effects or
the strong presence of habitual voting did not override the effect on
turnout that mail voting has. The same conclusion can be drawn for a
social theory of voting, which predicts no effect.

The best way to interpret my results is through a resources and
organizational lens, since this theory predicts a consistent, positive
effect of VBM on turnout because of the increase in capacity it gives to
individuals. In the trade-off between voting and not voting, whether
casting a ballot consumes time and effort seems to have had the largest
impact, which is why my models show such an effect. However, it should
be noted that this effect should be present in \emph{all} elections to
be convincing evidence for the resources electoral participation
paradigm. This is not the case. One explanation may be that my models
were sensitive to the vastly greater turnout that occurs during
presidential and midterm years, particularly since the differences in
mail voting percentage between counties was at most around 20\%. This,
again, leads to the conclusion that the lack of variability in my data
does not permit any more precise analysis.

The first step towards future research, directly resulting from this
problem, would be to get more data. This is very possible, but a task I
was unable to accomplish due to time constraints related to this
project. More data would allow for a comparative study of Colorado
elections before and after all major legislative electoral changes of
this century (2008, 2013). Another way to expand on my research would be
to look at even lower level elections, like municipal elections, school
board elections, or recall elections; voter history files contain data
on all of these races. A study with more data could use such races as
well for testing the hypotheses I set up in this thesis. Third, it is
possible to replicate my research here for other states like Oregon and
Washington, or for the specific counties in Arizona, Utah, or California
that have all-mail elections. Fourth, elections thankfully do not stop
happening; my research here can always be updated with data from the
2017 Coordinated Colorado election, or the 2018 Midterm that just took
place. I would caution that the same issue with data variability would
still exist in this case; there is a much greater need for data going
back, rather than new all-mail elections.

Apart from expanding my research in terms of data, another path would be
to impliment some of the methods I outline in the beginning of Chapter
4, like the Synthetic Control Group. Such methods would allow for
inferences to be made despite the data issues I encountered. Lastly, it
should be noted that VBM is just one of a series of electoral reforms
like Automatic Voter Registration (AVR), Early Voting, or Voter ID
Restrictions, all of which can potentially be tested using multilevel
models and data wrangling methods that I have applied in this thesis.

Another contribution of my thesis is my analysis of data wrangling, the
construction of multilevel general additive models for turnout, and the
accompanying \textit{R} package. To take these one by one, I have
provided arguments in favor of preferring multiple snapshots of
registration files rather than just the latest iteration of the record.
I have analyzed the pitfalls that exist in such documents, and given
specific examples on how this can be dealt with for the Colorado data
files. I have also provided a set of variable specification that can be
useful as indicators of the content of these data, or the potential uses
of voter registration files in other future studies. Finally, I have
presented potential future solutions to issues these data have with
variability, and ways to circumvent processing power limitations.

Additionally, I have meticulously gone through the creation of
multilevel general additive models of individual and county level
turnout. While due to data and processing power limitations I am unable
to run all these models to typical standards of convergence. This does
not mean that they present no value to future research. Quite the
contrary, future researchers just have to go through the data clean-up
stage, and then implement my models without having to construct them
from scratch. In particular, mixed effects and general additive models
are not widely used in such studies, making their presentation and
specification rather unique regardless of their application in this
piece of research.

Lastly, I provide an extensive library of code used to create this
document and the research I conduct. I have made an \textit{R}
package--which I named \texttt{riggd}--that includes more than a dozen
different functions that serve data wrangling and presentation purposes.
These functions are made for use on Colorado files, but require
relatively small amounts of changes to be applied to voter files from
around the US. I also provide code for all tables and graphics that are
included in this thesis on gitHub, which is a testament to the
reproducibility and future value of the research I conducted.

I recognize that, despite many obstacles in terms of data or computing
power, the outcome of this thesis being more constructive rather than
conclusive is to some extent my fault. There were many problems in this
thesis that I should have been aware of earlier in the process, which
may have allowed me to present more concrete results rather than a
series of tools and methods. However, in the combination of my existing
conclusions and the materials I have created through this process, it is
my belief that this thesis does in fact present a step forward in the
literature, and that it adds to existing quantitative elections studies
works. This is a small step, but it helps in our understanding of how
voters behave, what the actual results of election policy are, and how
to expand participation.

\appendix

\section{MCMC Estimation Processes for Multilevel
Models}\label{mcmc-estimation-processes-for-multilevel-models}

In statistical science a Markov Chain is a sequence of random variables
whose value depends on the value of the exact previous random variable.
In mathematical terms, this would be a sequence
\(\theta^{(1)}, \theta^{(2)}, \theta^{(3)}, ..., \theta^{(t)}\) where
\(\mathbb{P}(\Theta = \theta^{(t)}|\theta^{(n)}) = \mathbb{P}(\Theta = \theta^{(t)})\)
for \(n \in [1,t-2]\) but
\(\mathbb{P}(\Theta = \theta^{(t)}|\theta^{(n)})\) is dependent on
\(\theta^{(n)}\) for \(n = t-1\). A Markov Chain Monte Carlo simulation
uses Bayesian estimation to update each sequential estimate of
\(\theta\), leading it to converge to the true value being estimated
{[}@gelman\_data\_2006{]}.

Multilevel models can be estimated using MCMC sampling. Indicatively,
this appendix presents the construction and coding of two types of MCMC
samplers based on the Gibbs algorithm and Metropolis-Hastings algorithm.
The code and mathematical derivations are adapted to my models from
Gelman and Hill (2006).

\subsection{Gibbs Sampler for the County
Models}\label{gibbs-sampler-for-the-county-models}

The Gibbs algorithm works as follows:

\begin{enumerate}
  \item Choose a number of parallel simulation runs (chains). This number should be relatively low. In this example it is set to 3.
  \item For each chain do the following:
  \begin{enumerate}
    \item Initialize vector of parameters $\Theta^{(0)} = \{\theta^{(0)}_1\, \theta^{(0)}_2\, ..., \theta^{(0)}_n\}$
    \item Choose a number of iterations. For each iteration update every parameter in vector $\Theta^{(n_{iteration})}$, based on the values of vector $\Theta^{(n_{iteration} - 1)}$.
  \end{enumerate}
  \item Evaluate convergence between the chains.
\end{enumerate}

If convergence is poor, repeat for more iterations, or follow diagnostic
procedures. These are not specified here, but Gelman and Hill provide a
good overview {[}@gelman\_data\_2006; @gelman\_bayesian\_2003{]}.

\subsubsection{County Model 1 (Only Random County
Effects)}\label{county-model-1-only-random-county-effects}

A basic multilevel model with only group-level intercept mixed effects
can be written as follows:

\[y_i \ \sim \ N(a_{j[i]}, \sigma^2_y),\ i \in [1, n] \\ a_j \ \sim \ N(\mu_{\alpha}, \sigma^2_{\alpha}), \ j \in [1,J]\]
This specification is slightly different from that presented in Chapter
2. Here \(\alpha_{j[i]}\) is the coefficient for the group \(j\) that
individual \(i\) belongs to, \(\sigma_y, \sigma_{\alpha}\) the variances
of the individual and group level distributions respectively, and
\(\mu_{\alpha}\) the mean of the group-level distribution. In the case
of the most basic county-level model estimated in my thesis (County
Model 1), \(n = 704\) and \(J = 64\). Using Maximum Likelihood
Estimation, and given that:

\begin{equation}
  \alpha_j|y, \mu_{\alpha}, \sigma_y, \sigma_{\alpha} \ \sim \ N(\hat{\alpha_j}, V_j)
\end{equation}

we can obtain estimates:

\begin{equation}
\hat{\alpha_j} = \frac{\frac{n_{[j]}}{\sigma^2_y}\bar{y}_{[j]} + \frac{1}{\sigma^2{\alpha}}}{\frac{n_{[j]}}{\sigma^2_y} + \frac{1}{\sigma^2{\alpha}}},\ \ \ \ \  V_j = \frac{1}{\frac{n_{[j]}}{\sigma^2_y} + \frac{1}{\sigma^2{\alpha}}},
\end{equation}

where \(n_{[j]}\) is the number of observations for group j, and
\(\bar{y}_{[j]}\) is the mean response for group j. Using these
estimates and the common MLE estimates for variance and mean in a normal
distribution, it is possible to construct a Gibbs sampler for model
coefficients and errors. Step 2(b) in the Gibbs sampler would then be:

\begin{enumerate}
  \item Estimate $a_j, \ j\in[1,J]$ using equations (1), (2).
  \item Estimate $\mu_{\alpha}$ by drawing from $N(\frac{1}{J}\sum_{1}^{J}\alpha_j, \sigma_{\alpha}^2/J)$ using the previous values estimated in step 1.
  \item Estimate $\sigma_y^2$ as $\frac{\frac{1}{n}\sum_{1}^{n}(y_i - \alpha_{j[i]})^2}{X_{n-1}^2}$ where $X_{n-1}^2$ is a draw from a $\chi^2$ distribution with $n-1$ degrees of freedom.
  \item Estimate $\sigma_{\alpha}^2$ as $\frac{\frac{1}{J}\sum_{1}^{J}(\alpha_j - \mu_{\alpha})^2}{X_{J-1}^2}$ where $X_{n-1}^2$ is a draw from a $\chi^2$ distribution with $J-1$ degrees of freedom.
\end{enumerate}

While each step here seems relatively intuitive, the derivations behind
some of the details (like the chi-squared distribution) are complex MLE
processes and beyond the scope of this thesis. The R code for this
algorithm is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Gibbs sampler in R}
\NormalTok{a.update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  a.new <-}\StringTok{ }\KeywordTok{rep}\NormalTok{ (}\OtherTok{NA}\NormalTok{, J)}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{J)\{}
\NormalTok{    n.j <-}\StringTok{ }\KeywordTok{sum}\NormalTok{ (model_dt}\OperatorTok{$}\NormalTok{county}\OperatorTok{==}\NormalTok{cnt_vec[j])}
\NormalTok{    y.bar.j <-}\StringTok{ }\KeywordTok{mean}\NormalTok{ (model_dt}\OperatorTok{$}\NormalTok{turnout[model_dt}\OperatorTok{$}\NormalTok{county}\OperatorTok{==}\NormalTok{cnt_vec[j]])}
\NormalTok{    a.hat.j <-}\StringTok{ }\NormalTok{((n.j}\OperatorTok{/}\NormalTok{sigma.y}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\NormalTok{y.bar.j }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{sigma.a}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\NormalTok{mu.a)}\OperatorTok{/}
\StringTok{               }\NormalTok{(n.j}\OperatorTok{/}\NormalTok{sigma.y}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{sigma.a}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{    V.a.j <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{(n.j}\OperatorTok{/}\NormalTok{sigma.y}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{sigma.a}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{    a.new[j] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{ (}\DecValTok{1}\NormalTok{, a.hat.j, }\KeywordTok{sqrt}\NormalTok{(V.a.j))}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{ (a.new)}
\NormalTok{\}}
\NormalTok{mu.a.update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  mu.a.new <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{ (}\DecValTok{1}\NormalTok{, }\KeywordTok{mean}\NormalTok{(a), sigma.a}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(J))}
  \KeywordTok{return}\NormalTok{ (mu.a.new)}
\NormalTok{\}}
\NormalTok{sigma.y.update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  sigma.y.new <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((model_dt}\OperatorTok{$}\NormalTok{turnout}\OperatorTok{-}\NormalTok{a[model_dt}\OperatorTok{$}\NormalTok{county])}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{rchisq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{703}\NormalTok{))}
  \KeywordTok{return}\NormalTok{ (sigma.y.new)}
\NormalTok{\}}
\NormalTok{sigma.a.update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  sigma.a.new <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((a}\OperatorTok{-}\NormalTok{mu.a)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{rchisq}\NormalTok{(}\DecValTok{1}\NormalTok{,J}\OperatorTok{-}\DecValTok{1}\NormalTok{))}
  \KeywordTok{return}\NormalTok{ (sigma.a.new)}
\NormalTok{\}}

\NormalTok{J <-}\StringTok{ }\DecValTok{64}
\NormalTok{n.chains <-}\StringTok{ }\DecValTok{3}
\NormalTok{n.iter <-}\StringTok{ }\DecValTok{1000}
\NormalTok{sims <-}\StringTok{ }\KeywordTok{array}\NormalTok{ (}\OtherTok{NA}\NormalTok{, }\KeywordTok{c}\NormalTok{(n.iter, n.chains, J}\OperatorTok{+}\DecValTok{3}\NormalTok{))}
\KeywordTok{dimnames}\NormalTok{ (sims) <-}\StringTok{ }\KeywordTok{list}\NormalTok{ (}\OtherTok{NULL}\NormalTok{, }\OtherTok{NULL}\NormalTok{, }\KeywordTok{c}\NormalTok{ (}\KeywordTok{paste}\NormalTok{ (}\StringTok{"a["}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\NormalTok{J, }\StringTok{"]"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{""}\NormalTok{), }\StringTok{"mu.a"}\NormalTok{,}
   \StringTok{"sigma.y"}\NormalTok{, }\StringTok{"sigma.a"}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ (m }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n.chains)\{}
\NormalTok{  mu.a <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{ (}\DecValTok{1}\NormalTok{, }\KeywordTok{mean}\NormalTok{(model_dt}\OperatorTok{$}\NormalTok{turnout), }\KeywordTok{sd}\NormalTok{(model_dt}\OperatorTok{$}\NormalTok{turnout))}
\NormalTok{  sigma.y <-}\StringTok{ }\KeywordTok{runif}\NormalTok{ (}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\KeywordTok{sd}\NormalTok{(model_dt}\OperatorTok{$}\NormalTok{turnout))}
\NormalTok{  sigma.a <-}\StringTok{ }\KeywordTok{runif}\NormalTok{ (}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\KeywordTok{sd}\NormalTok{(model_dt}\OperatorTok{$}\NormalTok{turnout))}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n.iter)\{}
\NormalTok{    a <-}\StringTok{ }\KeywordTok{a.update}\NormalTok{ ()}
\NormalTok{    mu.a <-}\StringTok{ }\KeywordTok{mu.a.update}\NormalTok{ ()}
\NormalTok{    sigma.y <-}\StringTok{ }\KeywordTok{sigma.y.update}\NormalTok{ ()}
\NormalTok{    sigma.a <-}\StringTok{ }\KeywordTok{sigma.a.update}\NormalTok{ ()}
\NormalTok{    sims[t,m,] <-}\StringTok{ }\KeywordTok{c}\NormalTok{ (a, mu.a, sigma.y, sigma.a)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lccc@{}}
\caption{Gibbs sampler results for County Model 1
\label{tab:gibbs_1}}\tabularnewline
\toprule
\begin{minipage}[b]{0.26\columnwidth}\raggedright\strut
Calculated from\ldots{}\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
mu.a\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering\strut
sigma.y\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering\strut
sigma.a\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.26\columnwidth}\raggedright\strut
Calculated from\ldots{}\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\centering\strut
mu.a\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering\strut
sigma.y\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering\strut
sigma.a\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
Sampler\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
0.4695\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering\strut
0.2005\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering\strut
0.03772\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright\strut
Model\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\centering\strut
0.469\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering\strut
0.199\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering\strut
0.039\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

As is obvious from Table, the Gibbs sampler produces values very similar
to the ones given by an \textit{R} call of Model 1.

\subsubsection{County Model 2 (Random County Effects and County-Level
Predictors)}\label{county-model-2-random-county-effects-and-county-level-predictors}

With slight changes from the previous model the following is the
mathematical expression for a mixed effects model with group-level
predictors:

\[y_i \ \sim \ N(a_{j[i]}, \sigma^2_y),\ i \in [1, n] \\ a_j \ \sim \ N(U_j\gamma, \sigma^2_{\alpha}), \ j \in [1,J], \]

where \(U_j\) is a vector of predictor values for group \(j\), and
\(\gamma\) a vector of group-level coefficients, with the rest of the
parameters having the same designation as previously. Bear in mind that
the second of the previous expressions can also be written as:

\begin{equation}
\alpha_j = U_j\gamma + \eta_j, \ \ \eta_j \ \sim \ N(0, \sigma_{\alpha}^2)
\end{equation}

Updating the estimates used previously, it is again possible to
construct a Gibbs sampler for model coefficients and errors. Step 2(b)
in the Gibbs sampler in this case is:

\begin{enumerate}
  \item Estimate $a_j, \ j\in[1,J]$. Start by calculating $y_i^{temp} = y_i - U_{j[i]}\gamma$. Then calculate an estimate $\hat\eta_j$ and variance matrix $V_j$ from equations (1), (2), by replacing $\hat\alpha_j$ with $\hat\eta_j$ and $y$ with $y^{temp}$. Use $\eta_j \ \sim \ N(\hat\eta_j, V_j)$ to draw errors $\eta_j$ and then use (3) to estimate $\alpha_j$ for $j \in [1,J]$.
  \item Estimate $\gamma$ by first regressing $\alpha$ by predictor matrix $U$ to obtain $\hat\gamma$ and variance matrix $V_{\gamma}$. Then use distribution $\gamma_j \ \sim \ N(\hat\gamma_j, V_j)$ to obtain estimates for vector $\gamma$.
  \item Estimate $\sigma_y^2$ as $\frac{\frac{1}{n}\sum_{1}^{n}(y_i - \alpha_{j[i]})^2}{X_{n-1}^2}$ where $X_{n-1}^2$ is a draw from a $\chi^2$ distribution with $n-1$ degrees of freedom.
  \item Estimate $\sigma_{\alpha}^2$ as $\frac{\frac{1}{J}\sum_{1}^{J}(\alpha_j - U_j\gamma)^2}{X_{J-1}^2}$ where $X_{n-1}^2$ is a draw from a $\chi^2$ distribution with $J-1$ degrees of freedom.
\end{enumerate}

County Model 2, as presented in Chapter 4, includes two county-level
predictors: percentage of white residents and percentage of urban
population; this means that \(U = \{x^{\%white}, x^{\%urban}\}\).
Keeping this in mind the following code estimates the coefficients and
standard errors for Model 2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Gibbs sampler for a multilevel model with county predictors}

\NormalTok{a.update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  y.temp <-}\StringTok{ }\NormalTok{y }\OperatorTok{-}\StringTok{ }\NormalTok{U[county]}\OperatorTok{%*%}\NormalTok{g}
\NormalTok{  eta.new <-}\StringTok{ }\KeywordTok{rep}\NormalTok{ (}\OtherTok{NA}\NormalTok{, J)}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{J)\{}
\NormalTok{    n.j <-}\StringTok{ }\KeywordTok{sum}\NormalTok{ (county}\OperatorTok{==}\NormalTok{j)}
\NormalTok{    y.bar.j <-}\StringTok{ }\KeywordTok{mean}\NormalTok{ (y.temp[county}\OperatorTok{==}\NormalTok{j])}
\NormalTok{    eta.hat.j <-}\StringTok{ }\NormalTok{((n.j}\OperatorTok{/}\NormalTok{sigma.y}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{*}\NormalTok{y.bar.j}\OperatorTok{/}
\StringTok{                 }\NormalTok{(n.j}\OperatorTok{/}\NormalTok{sigma.y}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{sigma.a}\OperatorTok{^}\DecValTok{2}\NormalTok{))}
\NormalTok{    V.eta.j <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{(n.j}\OperatorTok{/}\NormalTok{sigma.y}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{sigma.a}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{    eta.new[j] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{ (}\DecValTok{1}\NormalTok{, eta.hat.j, }\KeywordTok{sqrt}\NormalTok{(V.eta.j))}
\NormalTok{  \}}
\NormalTok{  a.new <-}\StringTok{ }\NormalTok{U}\OperatorTok{%*%}\NormalTok{g }\OperatorTok{+}\StringTok{ }\NormalTok{eta.new}
  \KeywordTok{return}\NormalTok{ (a.new)}
\NormalTok{\}}

\NormalTok{g.update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  lm.}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{ (a }\OperatorTok{~}\StringTok{ }\NormalTok{U)}
\NormalTok{  g.new <-}\StringTok{ }\KeywordTok{sim}\NormalTok{ (lm.}\DecValTok{0}\NormalTok{, }\DataTypeTok{n.sims=}\DecValTok{1}\NormalTok{)}
  \KeywordTok{return}\NormalTok{ (g.new)}
\NormalTok{\}}
\NormalTok{sigma.y.update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  sigma.y.new <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((y}\OperatorTok{-}\NormalTok{a[county]}\OperatorTok{-}\NormalTok{X}\OperatorTok{%*%}\NormalTok{b)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{rchisq}\NormalTok{(}\DecValTok{1}\NormalTok{,n}\OperatorTok{-}\DecValTok{1}\NormalTok{))}
  \KeywordTok{return}\NormalTok{ (sigma.y.new)}
\NormalTok{\}}
\NormalTok{sigma.a.update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{()\{}
\NormalTok{  sigma.a.new <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{((a}\OperatorTok{-}\NormalTok{U}\OperatorTok{%*%}\NormalTok{g)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{rchisq}\NormalTok{(}\DecValTok{1}\NormalTok{,J}\OperatorTok{-}\DecValTok{1}\NormalTok{))}
  \KeywordTok{return}\NormalTok{ (sigma.a.new)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}


\end{document}
